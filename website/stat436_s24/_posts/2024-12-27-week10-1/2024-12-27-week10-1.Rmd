---
title: "Introduction to Dimensionality Reduction"
description: |
   Examples of high-dimensional data.
author:
  - name: Kris Sankaran
    affiliation: UW Madison
layout: post
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
library("knitr")
opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = FALSE)
```

_[Reading](https://idyll.pub/post/visxai-dimensionality-reduction-1dbad0a67a092b007c526a45/), [Recording](https://mediaspace.wisc.edu/media/Week%2010%20%5B1%5D%20Introduction%20to%20Dimensionality%20Reduction/1_z4difn4d), [Rmarkdown](https://github.com/krisrs1128/stat436_s24/blob/main/notes/2024-12-27-week10-1.Rmd)_

1. *High-dimensional data* are data where many features are collected for each
observation. These tend to be wide datasets with many columns. The name comes
from the fact that each row of the dataset can be viewed as a vector in a
high-dimensional space (one dimension for each feature). These data are common
in modern applications,

* Each cell in a genomics dataset might have measurements for hundreds of molecules.
* Each survey respondent might provide answers to dozens of questions.
* Each image might have several thousand pixels.
* Each document might have counts across several thousand relevant words.

2. For low-dimensional data, we could visually encode all the features in our
data directly, either using properties of marks or through faceting. In
high-dimensional data, this is no longer possible.

3. However, though there are many features associated with each observation, it
may still be possible to organize samples across a smaller number of meaningful,
derived features.

4. For example, consider the Metropolitan Museum of Art dataset, which contains
images of many artworks. Abstractly, each artwork is a high-dimensional object,
containing pixel intensities across many pixels. But it is reasonable to derive
a feature based on the average brightness.

```{r, fig.cap = "An arrangement of artworks according to their average pixel brightness, as given in the reading.", fig.align = "center"}
include_graphics("https://github.com/krisrs1128/stat436_s24/blob/main/exercises/figure/brightness.png?raw=true")
```

5. In general, manual feature construction can be difficult. Algorithmic
approaches try streamline the process of generating these maps by optimizing
some more generic criterion. Different algorithms use different criteria, which
we will review in the next couple of lectures.

```{r, fig.cap = "The dimensionality reduction algorithm in this animation converts a large number of raw features into a position on a one-dimensional axis defined by average pixel brightness. In general, we might reduce to dimensions other than 1D, and we will often want to define features tailored to the dataset at hand.", out.width = 400, fig.align = "center"}
include_graphics("https://github.com/krisrs1128/stat436_s24/blob/main/exercises/figure/dimred-animation.gif?raw=true")
```

6. Informally, the goal of dimensionality reduction techniques is to produce a
low-dimensional "atlas" relating members of a collection of complex objects.
Samples that are similar to one another in the high-dimensional space should be
placed near one another in the low-dimensional view. For example, we might want
to make an atlas of artworks, with similar styles and historical periods being
placed near to one another.
