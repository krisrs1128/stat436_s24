---
title: "Introduction to Topic Models"
description: |
  An overview of dimensionality reduction via topics.
author:
  - name: Kris Sankaran
    affiliation: UW Madison
layout: post
output:
  md_document:
    preserve_yaml: true
---

*[Reading](https://www.tidytextmining.com/topicmodeling.html),
[Recording](https://mediaspace.wisc.edu/media/Week%2011%20%5B1%5D%20Introduction%20to%20Topic%20Models/1_tqbn7u60),
[Rmarkdown](https://github.com/krisrs1128/stat479/blob/master/_posts/2021-03-30-week11-1/week11-1.Rmd)*

1.  Topic modeling is a type of dimensionality reduction method that is
    especially useful for high-dimensional count matrices. For example,
    it can be applied to,

    -   Text data analysis, where each row is a document and each column
        is a word. The *i**j*<sup>*t**h*</sup> entry contains the count
        of the *j*<sup>*t**h*</sup> word in the *i*<sup>*t**h*</sup>
        document.
    -   Gene expression analysis, where each row is a biological sample
        and each column is a gene. The *i**j*<sup>*t**h*</sup> entry
        measures the amount of gene *j* expressed in sample *i*.

2.  For clarity, we will refer to samples as documents and features as
    words. However, keep in mind that these methods can be used more
    generally – we will see a biological application three lectures from
    now.

3.  These models are useful to know about because they provide a
    compromise between clustering and PCA.

    -   In clustering, each document would have to be assigned to a
        single topic. In contrast, topic models allow each document to
        partially belong to several topics simultaneously. In this
        sense, they are more suitable when data do not belong to
        distinct, clearly-defined clusters.
    -   PCA is also appropriate when the data vary continuously, but it
        does not provide any notion of clusters. In contrast, topic
        models estimate *K* topics, which are analogous to a cluster
        centroids (though documents are typically a mix of several
        centroids).

4.  Without going into mathematical detail, topic models perform
    dimensionality reduction by supposing,

    -   Each document is a mixture of topics.
    -   Each topic is a mixture of words.

<img src="https://uwmadison.box.com/shared/static/3shdh2f5vqarkwjucmmebigj2rwm4wyh.png" alt="An overview of the topic modeling process. Topics are distributions over words, and the word counts of new documents are determined by their degree of membership over a set of underlying topics. In an ordinary clustering model, the bars for the memberships would have to be either pure purple or orange. Here, each document is a mixture." width="600" />
<p class="caption">
An overview of the topic modeling process. Topics are distributions over
words, and the word counts of new documents are determined by their
degree of membership over a set of underlying topics. In an ordinary
clustering model, the bars for the memberships would have to be either
pure purple or orange. Here, each document is a mixture.
</p>

1.  To illustrate the first point, consider modeling a collection of
    newspaper articles. A set of articles might belong primarily to the
    “politics” topic, and others to the “business” topic. Articles that
    describe a monetary policy in the federal reserve might belong
    partially to both the “politics” and the “business” topic.

2.  For the second point, consider the difference in words that would
    appear in politics and business articles. Articles about politics
    might frequently include words like “congress” and “law,” but only
    rarely words like “stock” and “trade.”

3.  Geometrically, LDA can be represented by the following picture. The
    corners of the simplex[1] represent different words (in reality,
    there would be *V* different corners to this simplex, one for each
    word). A topic is a point on this simplex. The closer the topic is
    to one of the corners, the more frequently that word appears in the
    topic.

<img src="http://2.bp.blogspot.com/-90BjNyRwkqk/T8bd9y7mUrI/AAAAAAAAAPg/H0Jdi-9RQ8s/s1600/LDA-f3.png" alt="A geometric interpretation of LDA, from the original paper by Blei, Ng, and Jordan." width="400" />
<p class="caption">
A geometric interpretation of LDA, from the original paper by Blei, Ng,
and Jordan.
</p>

1.  A document is a mixture of topics, with more words coming from the
    topics that it is close to. More precisely, a document that is very
    close to a particular topic has a word distribution just like that
    topic. A document that is intermediate between two topics has a word
    distribution that mixes between both topics. Note that this is
    different from a clustering model, where all documents would lie at
    exactly one of the corners of the topic simplex. Finally, note that
    the topics form their own simplex, since each document can be
    described as a mixture of topics, with mixture weights summing up to
    1.

[1] A simplex is the geometric object describing the set of probability
vectors over *V* elements. For example, if *V* = 3, then (0.1,0,0.9) and
(0.2,0.3,0.5) belong to the simplex, but not (0.3,0.1,9), since it sums
to a number larger than 1.
