---
title: "Principal Components Analysis I"
description: |
    Linear dimensionality reduction using PCA.
author:
  - name: Kris Sankaran
    affiliation: UW Madison
layout: post
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
library("knitr")
opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = TRUE)
```

_[Reading](https://juliasilge.com/blog/cocktail-recipes-umap/), [Recording](https://mediaspace.wisc.edu/media/Week%2010%20%5B2%5D%20Principal%20Components%20Analysis%20I/1_xc5q0v9t), [Rmarkdown](https://github.com/krisrs1128/stat436_s24/blob/main/notes/2024-12-27-week10-2.Rmd)_

```{r}
library(tidymodels)
library(readr)
```

1. In our last notes, we saw how we could organize a collection of images based
on average pixel brightness. We can think of average pixel brightness as a
derived feature that can be used to build a low-dimensional map.

2. We can partially automate the process of deriving new features. Though, in
general, finding the best way to combine raw features into derived ones is a
complicated problem, we can simplify things by restricting attention to,
  - Features that are linear combinations of the raw input columns.
  - Features that are orthogonal to one another.
  - Features that have high variance.

3. Restricting to *linear combinations* allows for an analytical solution. We
will relax this requirement when discussing UMAP.

4. *Orthogonality* means that the derived features will be uncorrelated with one
another. This is a nice property, because it would be wasteful if features were
redundant.

5. *High variance* is desirable because it means we preserve more of the
essential structure of the underlying data. For example, if you look at this 2D
representation of a 3D object, itâ€™s hard to tell what it is,

```{r, echo = FALSE, fig.cap = "What is this object?", fig.align = "center", out.width = 60}
include_graphics("https://web.stanford.edu/class/bios221/imgs/CAM3.png")
```

But when viewing an alternative reduction which has higher variance...

```{r, echo = FALSE, fig.cap = "Not so complicated now. Credit for this example goes to Professor Julie Josse, at Ecole Polytechnique.", fig.align = "center", out.width = 170, preview = TRUE}
include_graphics("https://web.stanford.edu/class/bios221/imgs/CAM4.png")
```
	
6. Principal Components Analysis (PCA) is the optimal dimensionality reduction
under these three restrictions, in the sense that it finds derived features with
the highest variance. Formally, PCA finds a matrix $\Phi \in \mathbb{R}^{D
\times K}$ and a set of vector $z_{i} \in \mathbb{R}^{K}$ such that $x_{i}
\approx \Phi z_{i}$ for all $i$. The columns of $\Phi$ are called principal
components, and they specify the structure of the derived linear features. The
vector $z_{i}$ is called the score of $x_{i}$ with respect to these components.
The top component explains the most variance, the second captures the next most,
and so on.

7. For example, if one of the columns of $\Phi$ was equal to $\left(\frac{1}{D},
\dots, \frac{1}{D}\right)$, then that feature computes the average of all
coordinates (e.g., to get average brightness), and the corresponding $z_{i}$
would be a measure of the average brightness of sample $i$.

8. Geometrically, the columns of $\Phi$ span a plane that approximates the data.
The $z_{i}$ provide coordinates of points projected onto this plane.

```{r, echo = FALSE, fig.cap = "PCA finds a low-dimensional linear subspace that closely approximates the high-dimensional data."}
include_graphics("https://github.com/krisrs1128/stat436_s24/blob/main/exercises/figure/pca_projection.png?raw=true")
```

9. In R, PCA can be conveniently implemented using the tidymodels package. We
will see a base R implementation in the next lecture. The
[dataset](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-26/readme.md)
below contains properties of a variety of cocktails, from the Boston Bartender's
guide. The first two columns are qualitative descriptors, while the rest give
numerical ingredient information.

```{r}
cocktails_df <- read_csv("https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv")
cocktails_df[, 1:6]
```
10. The `pca_rec` object below defines a tidymodels recipe for performing PCA.
Computation of the lower-dimensional representation is deferred until `prep()`
is called. This delineation between workflow definition and execution helps
clarify the overall workflow, and it is typical of the tidymodels package.

```{r}
pca_rec <- recipe(~., data = cocktails_df) %>%
  update_role(name, category, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

pca_prep <- prep(pca_rec)
```

11. The `step_normalize` call is used to center and scale all the columns. This
is needed because otherwise columns with larger variance will have more weight
in the final dimensionality reduction, but this is not conceptually meaningful.
For example, if one of the columns in a dataset were measuring length in
kilometers, then we could artificially increase its influence in a PCA by
expressing the same value in meters. To achieve invariance to this change in
units, it would be important to normalize first.

12. We can `tidy` each element of the workflow object. Since PCA was the second
step in the workflow, the PCA components can be obtained by calling tidy with the
argument "2." The scores of each sample with respect to these components can be
extracted using `juice.` The amount of variance explained by each dimension is
also given by `tidy`, but with the argument `type = "variance"`. We'll see how
to visualize and interpret these results in the next lecture.

```{r, fig.cap = "The first five principal components are linear combinations of features."}
tidy(pca_prep, 2)
```

```{r, fig.cap = "The coordinates of a few samples with respect to the princpal components above."}
juice(pca_prep)
```
```{r, fig.cap = "Total variance explained by each component of the PCA."}
tidy(pca_prep, 2, type = "variance")
```
